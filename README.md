# NLP Model Implementations

This repository contains three Jupyter notebooks demonstrating different NLP model implementations: Transformers, Encoder-Decoder, and Word2Vec.

## Project Descriptions

### 1. Transformers Implementation
- **Notebook**: `Project_Transformers.ipynb`
- **Description**: Explores key components of Transformer architecture and its implementation using TensorFlow/Keras.
- **Code Cells**: 10
- **Markdown Cells**: 5
- **Initial Code**: TensorFlow and Keras library imports.

### 2. Encoder-Decoder Implementation
- **Notebook**: `Project_Encoder_Decoder.ipynb`
- **Description**: Discusses Encoder-Decoder architecture and its usage in NLP tasks.
- **Code Cells**: 11
- **Markdown Cells**: 5
- **Initial Code**: TensorFlow and Keras library imports.

### 3. Word2Vec Implementation
- **Notebook**: `Project_Word2Vec.ipynb`
- **Description**: Implements Word2Vec for word representation in NLP.
- **Code Cells**: 19
- **Markdown Cells**: 7
- **Initial Code**: Uses Google Colab (drive mount).

## Requirements
Ensure the following dependencies are installed before running the notebooks:
```bash
pip install tensorflow gensim numpy pandas matplotlib
```

## How to Use
1. Open each notebook in Jupyter Notebook or Google Colab.
2. Run the cells sequentially to understand and experiment with the model implementations.
3. Modify the code as needed to explore different hyperparameters and configurations.

## Contributions
Feel free to contribute by improving implementations or adding new NLP models.

## License
This project is open-source and available for educational and research purposes.

